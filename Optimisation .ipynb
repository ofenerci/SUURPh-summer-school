{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to do optimization better than \"classical\" mathematician? (scipy.optimize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use <a href= \"https://docs.scipy.org/doc/scipy/reference/optimize.html#module-scipy.optimize\"> scipy.optimize </a> package for black-box optimization: we do not rely on the mathematical expression of the function that we are optimizing. Note that this expression can often be used for more efficient, non black-box, optimization.\n",
    "<b>Prerequisites:</b>\n",
    "    1. Numpy, Scipy\n",
    "    2. matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to start: get to know your problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all optimization problems are equal. Knowing your problem enables you to choose the right tool. Remember to account for:\n",
    "    \n",
    "    1. dimensionality of the problem (why?);\n",
    "    2. convex vs non-convex optimization (what is easier?);\n",
    "    3. smooth vs non-smooth problems (what is easier?); \n",
    "    4. noisy vs exact data;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Characterise each of the below functions and mention which of them are easier to optimize:\n",
    "<img src=\"images/noisy.png\"> \n",
    "<img src=\"images/nonsmooth.png\"> \n",
    "<img src=\"images/smooth.png\"> \n",
    "<img src=\"images/convex.png\"> \n",
    "<img src=\"images/non-convex.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is available? Overview of different optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Getting started: 1D optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use scipy.optimize.brent() to minimize 1D functions. It combines a bracketing strategy (golden section) with a parabolic approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6999999997839409"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import optimize\n",
    "def f(x):\n",
    "    return -np.exp(-(x - .7)**2)\n",
    "x_min = optimize.brent(f)  # It actually converges in 9 iterations!\n",
    "x_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    x = np.linspace(-1, 2, 100)\n",
    "    y = -np.exp(-(x - .7)**2)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel('$x$')\n",
    "    plt.ylabel('$\\exp(-(x-0.7)^2)$')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the Rosenbrock's function, which designed specifically for testing optimization techniques, it has curved, narrow valley.\n",
    "<img src=\"images/rosenbock.png\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gradient based methods: Conjugate gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent basically consists in taking small steps in the direction of the gradient, that is the direction of the steepest descent.\n",
    "\n",
    "Methods based on conjugate gradient are named with ‘cg’ in scipy. The simple conjugate gradient method to minimize a function is ``` scipy.optimize.fmin_cg():```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 13\n",
      "         Function evaluations: 120\n",
      "         Gradient evaluations: 30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.99998968,  0.99997855])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):   # The rosenbrock function\n",
    "    return .5*(1 - x[0])**2 + (x[1] - x[0]**2)**2\n",
    "optimize.fmin_cg(f, [2, 2])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These methods need the gradient of the function. They can compute it, but will perform better if you can pass them the gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 13\n",
      "         Function evaluations: 30\n",
      "         Gradient evaluations: 30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.99999199,  0.99998336])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fprime(x):\n",
    "    return np.array((-2*.5*(1 - x[0]) - 4*x[0]*(x[1] - x[0]**2), 2*(x[1] - x[0]**2)))\n",
    "optimize.fmin_cg(f, [2, 2], fprime=fprime)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the function has only been evaluated 30 times, compared to 120 without the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gradient based methods: Newton and quasi-newton methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newton methods use a local quadratic approximation to compute the jump direction. For this purpose, they rely on the 2 first derivative of the function: the gradient and the Hessian.\n",
    "\n",
    "In scipy, the Newton method for optimization is implemented in ``` scipy.optimize.fmin_ncg()``` (cg here refers to that fact that an inner operation, the inversion of the Hessian, is performed by conjugate gradient). ``` scipy.optimize.fmin_tnc()``` can be use for constraint problems, although it is less versatile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 9\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 51\n",
      "         Hessian evaluations: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):   # The rosenbrock function\n",
    "    return .5*(1 - x[0])**2 + (x[1] - x[0]**2)**2\n",
    "def fprime(x):\n",
    "    return np.array((-2*.5*(1 - x[0]) - 4*x[0]*(x[1] - x[0]**2), 2*(x[1] - x[0]**2)))\n",
    "optimize.fmin_ncg(f, [2, 2], fprime=fprime)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that compared to a conjugate gradient (above), Newton’s method has required less function evaluations, but more gradient evaluations, as it uses it to approximate the Hessian. Let’s compute the Hessian and pass it to the algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 9\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 19\n",
      "         Hessian evaluations: 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hessian(x): # Computed with sympy\n",
    "    return np.array(((1 - 4*x[1] + 12*x[0]**2, -4*x[0]), (-4*x[0], 2)))\n",
    "optimize.fmin_ncg(f, [2, 2], fprime=fprime, fhess=hessian)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note:</b> At very high-dimension, the inversion of the Hessian can be costly and unstable (large scale > 250).\n",
    "\n",
    "<b>Note:</b> Newton optimizers should not to be confused with Newton’s root finding method, based on the same principles, ``` scipy.optimize.newton().```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>BFGS:</b> BFGS (Broyden-Fletcher-Goldfarb-Shannon algorithm) refines at each step an approximation of the Hessian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 16\n",
      "         Function evaluations: 24\n",
      "         Gradient evaluations: 24\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.00000017,  1.00000026])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):   # The rosenbrock function\n",
    "    return .5*(1 - x[0])**2 + (x[1] - x[0]**2)**2\n",
    "def fprime(x):\n",
    "    return np.array((-2*.5*(1 - x[0]) - 4*x[0]*(x[1] - x[0]**2), 2*(x[1] - x[0]**2)))\n",
    "optimize.fmin_bfgs(f, [2, 2], fprime=fprime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradient-less methods: Simplex method (the Nelder-Mead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplex algorithm is probably the simplest way to minimize a fairly well-behaved function. It requires only function evaluations and is a good choice for simple minimization problems. However, because it does not use any gradient evaluations, it may take longer to find the minimum.\n",
    "\n",
    "The Nelder-Mead algorithms is a generalization of dichotomy approaches to high-dimensional spaces. The algorithm works by refining a simplex, the generalization of intervals and triangles to high-dimensional spaces, to bracket the minimum.\n",
    "\n",
    "Strong points: it is robust to noise, as it does not rely on computing gradients. Thus it can work on functions that are not locally smooth such as experimental data points, as long as they display a large-scale bell-shape behavior. However it is slower than gradient-based methods on smooth, non-noisy functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scipy, ```scipy.optimize.fmin()``` or minimize routine with ```method='nelder-mead'``` implements the Nelder-Mead approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 46\n",
      "         Function evaluations: 91\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.99998568,  0.99996682])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):   # The rosenbrock function\n",
    "    return .5*(1 - x[0])**2 + (x[1] - x[0]**2)**2\n",
    "optimize.fmin(f, [2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another optimization algorithm that needs only function calls to find the minimum is Powell‘s method available by setting ```method='powell'``` in minimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Global optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your problem does not admit a unique local minimum (which can be hard to test unless the function is convex), and you do not have prior information to initialize the optimization close to the solution, you may need a global optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` scipy.optimize.brute()``` evaluates the function on a given grid of parameters and returns the parameters corresponding to the minimum value. The parameters are specified with ranges given to ``` numpy.mgrid```. By default, 20 steps are taken in each direction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.00001462,  1.00001547])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):   # The rosenbrock function\n",
    "    return .5*(1 - x[0])**2 + (x[1] - x[0]**2)**2\n",
    "optimize.brute(f, ((-1, 2), (-1, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Practical guide to optimization with scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Without knowledge of the gradient:</b>\n",
    "* In general, prefer ``` BFGS (scipy.optimize.fmin_bfgs())```, even if you have to approximate numerically gradients\n",
    "* On well-conditioned problems, gradient-free methods, work well in high dimension, but they collapse for ill-conditioned problems.\n",
    "\n",
    "***\n",
    "\n",
    "<b>With knowledge of the gradient:</b>\n",
    "\t\n",
    "* Use BFGS ``` (scipy.optimize.fmin_bfgs())```.\n",
    "* Computational overhead of BFGS is larger than that of conjugate gradient. On the other side, BFGS usually needs less function evaluations than CG. Thus conjugate gradient method is better than BFGS at optimizing computationally cheap functions.\n",
    "\n",
    "***\n",
    "\n",
    "<b>With knowledge of the Hessian:</b>\n",
    "\n",
    "* If you can compute the Hessian, prefer the Newton method ``` (scipy.optimize.fmin_ncg()).```\n",
    "\n",
    "***\n",
    "\n",
    "<b>If you have noisy measurements:</b>\n",
    "\n",
    "* Use gradient-free methods like simplex method ```(scipy.optimize.fmin()).```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
